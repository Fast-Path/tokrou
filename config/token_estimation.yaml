# Token estimation rules for different query types and complexities

# System prompt sizes (tokens) for each model
system_prompts:
  gemini: 150      # Gemini system prompt
  coder: 200       # Coder system prompt with code context
  grok: 180        # Grok system prompt
  classifier: 50   # Lightweight classifier prompt

# Input token ranges [min, max] by query type and complexity
input_ranges:
  visual:
    simple: [100, 300]      # Simple image queries
    medium: [300, 800]      # Medium complexity visual tasks
    complex: [800, 2000]    # Complex visual analysis
  
  code:
    simple: [50, 200]       # Simple code snippets
    medium: [200, 600]      # Medium code generation
    complex: [600, 1500]    # Complex code refactoring
  
  research:
    simple: [100, 400]      # Simple research queries
    medium: [400, 1000]     # Medium research tasks
    complex: [1000, 3000]   # Complex research with multiple sources

# Output token multipliers by complexity
# output_tokens = input_tokens * multiplier
output_multipliers:
  simple: 0.5    # Simple queries produce shorter responses
  medium: 1.0    # Medium queries produce similar length responses
  complex: 1.5   # Complex queries produce longer responses

# Wrong model penalty multiplier
# Applied when query is routed to incorrect model
wrong_model_penalty: 1.3  # 30% more tokens due to suboptimal handling
